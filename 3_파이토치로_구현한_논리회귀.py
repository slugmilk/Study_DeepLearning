# -*- coding: utf-8 -*-
"""3. 파이토치로 구현한 논리회귀.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u3IjsQ4u-0CHSxZxB7M44DLW_0Z8nYFM

# **1. 단항 논리회귀(Logistic Regression)**
* 분류를 할 때 사용하며, 선형 회귀 공식으로부터 나왔기 때문에 논리회귀라는 이름이 붙여짐
* 회귀 분석을 기반으로 하지만 분류 문제에 사용
* 주로 시그모이드 함수를 사용
  * 예측값을 0에서 1사이의 값이 되도록 만듦
  * 0에서 1사이의 연속된 값을 출력으로 하기 때문에 보통 0.5(임계값)를 기준으로 구분
  * S자 곡선을 그리므로 미분 가능한 형태를 가지고 있어서 최적화가 용이
  * 주어진 입력값이 특정 클래스에 속할 확률을 계산. 이진 분류 문제를 해결
  <center><img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F8IXlq%2FbtsDKOmidD4%2FPfevddwbOhuVQHJG6YFbS1%2Fimg.png' width=600></center>
  <center><img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fsib2q%2FbtsDJkfaBMy%2FpkKQYrLYXHVDODrXvQNqK0%2Fimg.png' width=600></center>

* 논리회귀는 입력 데이터 x에 대한 선령 결합 계산 -> 그 결과를 시그모이드 함수에 통과 시켜 출력값을 0과 1 사이의 값으로 변환 -> 이 값을 특정 클래스에 속할 확률로 해석
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn

x = torch.tensor([1.0, 2.0, 3.0])
w = torch.tensor([0.1, 0.2, 0.3])
b = torch.tensor(0.5)

# z = w1x1 + w2x2 + w3x3 + b
z = torch.dot(w, x) + b # dot: 내적
z

sigmoid = nn.Sigmoid()
output = sigmoid(z)
output

import torch.optim as optim
import matplotlib.pyplot as plt

torch.manual_seed(2024)

x_train = torch.FloatTensor([[0], [1], [3], [5], [8], [11], [15], [20]])
y_train = torch.FloatTensor([[0], [0], [0], [0], [1], [1], [1], [1]])
print(x_train.shape)
print(y_train.shape)

plt.figure(figsize=(8,5))
plt.scatter(x_train, y_train)

model = nn.Sequential(
    nn.Linear(1, 1),
    nn.Sigmoid()
)

model

list(model.parameters()) # W: 0.0634 b: 0.6625

"""# **2. 비용함수**
* Binary Cross Entropy
* 논리회귀에서는 nn.BCELoss() 함수를 사용하여 Loss 계산
* 1번 시그마, 2번 시그마 중에서 1번 시그마는 정답이 참이었을 때 부분, 2번 시그마는 정답이 거짓이었을 때 부분

<img src='https://i.imgur.com/tvCuEmh.png' width='500'>
"""

y_pred = model(x_train)
y_pred

loss = nn.BCELoss()(y_pred, y_train)
loss

optimizer = optim.SGD(model.parameters(), lr=0.01)

epochs = 1000

for epoch in range(epochs + 1):
    y_pred = model(x_train)
    loss = nn.BCELoss()(y_pred, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % 100 == 0:
        print(f'Epoch: {epoch}/{epochs} Loss: {loss:.6f}')

list(model.parameters())

x_test = torch.FloatTensor([[10]])
y_pred = model(x_test)
y_pred

# 임계치 설정하기
# 0.5보다 크거나 같으면 1
# 0.5보다 작으면 0
y_bool = (y_pred >= 0.5).float()
y_bool

"""# **3. 다항 논리회귀**"""

x_train = [[1, 2, 1, 1],
           [2, 1, 3, 2],
           [3, 1, 3, 4],
           [4, 1, 5, 5],
           [1, 7, 5, 5],
           [1, 4, 5, 9],
           [1, 7, 7, 7],
           [2, 8, 7, 8]]
# 변수 4개
y_train = [0, 0, 0, 1, 1, 1, 2, 2]
# 클래스 3

x_train = torch.FloatTensor(x_train)
y_train = torch.LongTensor(y_train)
print(x_train.shape)
print(y_train.shape)

model = nn.Sequential(
    nn.Linear(4, 3) # 입력 4개 출력 3개
    # 앞에 단항에서 출력이 1인 이유는 Sigmoid로 0또는 1이 될 확률을 반환히기 때문에
    # 이번에는 각각의 출력에 대한 확률을 반환(확률 총 합은 1)
)
model

y_pred = model(x_train)
y_pred

"""### 3-1. CrossEntropyLoss
* 교차 엔트로피 손실 함수는 Pytorch에서 제공하는 손실 함수 중 하나로 다중 클래스 분류 문제에서 사용
* 소프트맥스 함수와 교차 엔트로피 손실 함수를 결합한 형태
* 소프트맥스 함수를 적용하여 각 클래스에 대한 확률 분포를 얻음
* 각 클래스에 대한 로그 확률을 계산
* 실제 라벨과 예측 확률의 로그 값 간의 차이를 계산
* 계산된 차이의 평균을 계산하여 최종 손실 값을 얻음

 <center><img src='https://velog.velcdn.com/images%2Fyuns_u%2Fpost%2F712f82c2-ea39-4a3f-85d7-940e159bd15c%2Fimage.png'></center>

### 4-2. SoftMax
* 다중 클래스 분류 문제에서 사용되는 함수로 주어진 입력 벡터의 값을 확률 분포로 변환
* 각 클래스에 속할 확률을 계산할 수 있으며, 각 요소를 0과 1사이의 값으로 변환하여 이 값들의 합은 항상 1이 되도록 함
* 각 입력 값에 대해 지수함수를 적용
* 지수 함수를 적용한 모든 값의 합을 계산한 후, 각 지수의 합으로 나누어 정규화를 함
* 정규화를 통해 각 값은 0과 1사이의 확률 값으로 출력|

<center><img src='https://blog.kakaocdn.net/dn/7o3ns/btqvQDIyhq4/FYgVfbO6NaJrkc7y11f440/img.png'></center>
"""

loss = nn.CrossEntropyLoss()(y_pred, y_train)
loss

optimizer = optim.SGD(model.parameters(), lr=0.01)

epochs = 10000

for epoch in range(epochs + 1):
  y_pred = model(x_train)
  loss = nn.CrossEntropyLoss()(y_pred, y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  if epoch % 100 == 0:
    print(f'Epoch: {epoch}/{epochs} Loss: {loss: .6f}')

x_test = torch.FloatTensor([[1, 7, 8, 7]])
y_pred = model(x_test)
y_pred

# nn.Softmax(1)
# 소프트맥스 함수를 적용할 차원을 지정
# dim=0일 경우 첫 번째 차원을 따라 소프트 맥스를 계산 (행끼리 계산)
# dim=1일 경우 두 번째 차원을 따라 소프트 맥스를 계산 (열끼리 계산)

y_prob = nn.Softmax(1)(y_pred)
y_prob

print(f'0일 확률: {y_prob[0][0]:.2f}')
print(f'1일 확률: {y_prob[0][1]:.2f}')
print(f'2일 확률: {y_prob[0][2]:.2f}')

torch.argmax(y_prob, axis=1) # 확률이 가장 높은 값

"""# **4. 경사 하강법의 종류**

### 4-1. 배치 경사 하강법
* 가장 기본적인 경사 하강법(Vanilla Gradient Descent)
* 데이터셋 전체를 고려하여 손실함수를 계산
* 한 번의 Epoch에 모든 파라미터 업데이트를 단 한 번만 수행
* 파라미터 업데이트힐 때 한 번의 전체 데이터셋을 고려하기 때문에 모델 학습 시 많은 시간과 메모리가 필요하다는 단점이 있음

### 4-2. 확률적 경사 하강법
* 확률적 경사 하강법(Stochastoc Gradient Descent)은 배치 경사 하강법이 모델 학습 시 많은 시간과 메모리가 필요하다는 단점을 보완하기 위해 제안된 기법
* batch size를 1로 설정하여 파라미터를 업데이트 하기 때문에 배치 경사 하강법보다 훨씬 빠르고 적은 메모리로 학습을 진행
* 파라미터 값의 업데이트 폭이 불안정하기 때문에 정확도가 낮은 경우가 생길 수 있음

### 4-3. 미니 배치 경사 하강법
* 미니 배치 경사 하강법(Mini-Batch Gradient Descent)은 Batch Size를 설정한 size로 사용
* 배치 경사 하강법보다 모델 속도가 빠르고, 확률적 경사 하강법보다 안정적인 장점이 있음
* 딥러닝 분야에서 가장 많이 활용되는 경사 하강법
* 일반적으로 Batch Size를 4, 8, 16, 32, 64, 128과 같이 2의 n제곱에 해당하는 값으로 사용하는게 관례적

# **5. 경사 하강법의 여러가지 알고리즘**

### 5-1. SGD(확률적 경사 하강법)
* 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법

### 5-2. 모멘텀(Momentum)
* 경사 하강법의 단점을 보완하기 위해 도입된 알고리즘
* 관성이라는 물리학 법칙을 응용한 방법
* 접선의 기울기에 한 시점 이전의 접선의 기울기 값을 일정한 비율만큼 반영
* 이전 기울기의 이동 평균을 사용하여 현재 기울기를 업데이트
* 가속도를 제공하여, 경사 하강법보다 빠르게 최소값에 도달할 수 있음

### 5-3. 아다그라드(Adagrad)
* 모든 매개변수에 동일한 학습률(lr)을 적용하는 것은 비효율적이다라는 생각에서 만들어진 학습 방법
* 처음에는 크게 학습하다가 조금씩 작게 학습시킴
* 각 파라미터에 맞춤형 학습률을 적용하는 방법
* 희소한 데이터에서 유리함
* 시간이 지남에 따라 학습률이 계속 감소하여 학습을 멈출 수 있음

### 5-4. 아담(Adam)
* 모멘텀 + 아다그라드
* 각 매개변수에 대해 적응형 학습률을 적용하며, 과거의 기울기 정보를 활용해 현재의 학습률을 조절
* AdamW: Adam의 변형으로 L2정규화(가중치 감쇠)를 별도로 처리하여 더 나은 일반화 성능을 제공, L2 정규화가 학습률 조정과 섞여 불안정한 학습을 초래할 수 있는 문제를 해결

# **6. 와인 품종 예측하기**
* sklearn.datasets.load_wine: 이탈리아의 같은 지역에서 재배된 세가지 다른 품종으로 만든 와인을 화학적으로 분석한 결과에 대한 데이터셋
* 13개의 성분을 분석하여 어떤 와인인지 구별하는 모델을 구축
* 데이터를 섞은 후 train 데이터를 80%, test 데이터를 20%로 하여 사용
* Adam을 사용
    * optimizer = optim.Adam(model.parameters(), lr=0.01)
* 테스트 데이터의 0번 인덱스가 어떤 와인인지 알아보자. 정확도를 출력
"""

from sklearn.datasets import load_wine

x_data, y_data = load_wine(return_X_y=True, as_frame=True)

x_data

y_data

x_data = torch.FloatTensor(x_data.values)
y_data = torch.LongTensor(y_data.values)
print(x_data.shape)
print(y_data.shape)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=2024)

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

model = nn.Sequential(
    nn.Linear(13, 3)
)

optimizer = optim.Adam(model.parameters(), lr=0.01)

epochs = 1000

for epoch in range(epochs + 1):
  y_pred = model(x_train)
  loss = nn.CrossEntropyLoss()(y_pred, y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  if epoch % 100 == 0:
    y_prob = nn.Softmax(1)(y_pred)
    y_pred_index = torch.argmax(y_prob, axis=1)
    y_train_index = y_train
    accuracy = (y_pred_index == y_train_index).float().sum() / len(y_train) * 100
    print(f'Epoch {epoch:4d}/{epochs} Loss:{loss: .6f} Accuracy: {accuracy: .2f}%')

y_pred = model(x_test)
y_pred[:5]

y_prob = nn.Softmax(1)(y_pred)
y_prob[:5]

print(f'0번 품종일 확률: {y_prob[0][0]:.2f}')
print(f'1번 품종일 확률: {y_prob[0][1]:.2f}')
print(f'2번 품종일 확률: {y_prob[0][2]:.2f}')

y_pred_index = torch.argmax(y_prob, axis=1)
accuracy = (y_test == y_pred_index).float().sum() / len(y_test) * 100
print(f'테스트 정확도는 {accuracy: .2f}% 입니다!')

