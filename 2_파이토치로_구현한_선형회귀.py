# -*- coding: utf-8 -*-
"""2. 파이토치로 구현한 선형회귀

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wKpZXERisiLNSym3e2onKetriKb0zjEQ

# **1. 단항 선형 회귀**
* 한 개의 입력이 들어가서 한 개의 출력이 나오는 구조
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

torch.manual_seed(2024)

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
print(x_train, x_train.shape)
print(y_train, y_train.shape)

plt.figure(figsize=(6, 4))
plt.scatter(x_train, y_train)

# y = Wx + b
model = nn.Linear(1, 1) # 데이터 하나 들어오면 출력 하나
model

y_pred = model(x_train) # 아직은 학습 안돼있음
y_pred

list(model.parameters()) # W: 0.0634, b: 0.6625
# y = Wx + b
# x=1, 0.0634*1 + 0.6625 = 0.7259
# x=2, 0.0634*2 + 0.6625 = 0.7893

((y_pred - y_train)**2).mean() # 오차 계산

loss = nn.MSELoss()(y_pred, y_train) # 함수로 오차 계산
loss

mse = nn.MSELoss() # 객체 생성
mse(y_pred, y_train)

"""# **2. 최적화(Optimization)**
* 학습 모델의 손실함수(loss function)의 최소값을 찾아가는 과정
* 학습 데이터를 입력하여 파라미터를 걸쳐 예측 값을 받음 -> 예측값과 실제 정답과의 차이를 비교하는 것이 손실함수이고, 예측값과 실젯값의 차이를 최소화하는 파라미터를 찾는 과정이 최적화

### 2-1. 경사하강법(Gradient Descent)
* 딥러닝 알고리즘 학습 시 사용되는 최적화 방법 중 하나
* 최적화 알고리즘을 통해 W와 b를 찾아내는 과정을 '학습'이라고 부름

<center><img src='https://i.imgur.com/0fW4LTG.png' width=600></center>

### 2-2. 학습률(Learning rate)
* 한 번의 W를 움직이는 거리(increment step)
* 0 ~ 1 사이의 실수
* 학습률이 너무 크면 한 지점으로 수렴하는 것이 아니라 발산할 가능성이 존재
* 학습률이 너무 작으면 수렴이 늦어지고, 시작점을 어디로 잡느냐에 따라 수렴 지점이 달라짐

<center><img width=800 src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FedThAP%2FbtrYAnoERmh%2FDXVtwO2M7GkM4l5dRcFxr0%2Fimg.png'>

<center><img width=800 src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbxxkZk%2FbtrYzEEt8bd%2FUL9zBuxrdTaIBUtYS5kPI0%2Fimg.png'></center>

### 2-3. 경사 하강법의 한계
* 많은 연산량과 컴퓨터 자원을 소모
* 데이터(입력값) 하나가 모델을 지날 때마다 모든 가중치를 한 번씩 업데이트 함
* 가중치가 적은 모델의 경우 문제가 없으나, 모델의 가중치가 매우 많다면 모든 가중치에 대해 연산을 적용하기 때문에 많은 연산량을 요구
* Global Minimum은 목표 함수 그래프 전체를 고려했을 때 최솟값을 의미하고, Local Minimum은 그래프 내 일부만 고려했을 때 최솟값을 의미 -> 경사 하강법으로 최적의 값인 줄 알았던 값이 Local Minimum으로 결과가 나올 수 있음
"""

# SGD(Stochastic Gradient Descent)
# 랜덤하게 데이터를 하나씩 뽑아서 loss를 만듦
# 데이터를 넣고 다시 데이터를 뽑고 반복
# 빠르게 방향을 결정
optimizer = optim.SGD(model.parameters(), lr=0.01) # 옵티마이저 객체

loss = nn.MSELoss()(y_pred, y_train)

# optimizer를 초기화
# loss.backward() 호출될 때 초기설정은 gradient를 더해주는 것으로 되어 있음
# 학습 loop를 돌 때 이상적으로 학습이 이루어지기 위해서 한 번의 학습이 완료되면 graidient를 항상 0으로 만들어줘야 함
optimizer.zero_grad()
# 역전파: 비용 함수를 미분하여 gradient 계산
loss.backward()
# 가중치 업데이트: 계산된 gradient를 사용하여 파라미터를 업데이트
optimizer.step()

# list(model.parameters()) # W: 0.0634, b: 0.6625
list(model.parameters()) # W: 0.2177 b: 0.7267

# 반복 학습을 통해 오차가 있는 W, b를 수정하면서 오차를 계속 줄여나감
# epochs: 반복 학습 횟수(에포크)
epochs = 1000

for epoch in range(epochs + 1):
    y_pred = model(x_train)
    loss = nn.MSELoss()(y_pred, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % 100 == 0:
        print(f'Epoch: {epoch}/{epochs} Loss: {loss:.6f}')

print(list(model.parameters())) # W: 1.9499, b: 0.1138

x_test = torch.FloatTensor([[5]])
y_pred = model(x_test)
y_pred

"""# **3. 다중 선형 회귀**
* 여러 개의 입력이 들어가서 한 개의 출력이 나오는 구조
"""

X_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[220], [270], [265], [290], [200]])

print(X_train, X_train.shape)
print(y_train, y_train.shape)

# y = w1x1 + w2x2 + w3x3 + b
model = nn.Linear(3, 1) # 입력3개 출력1개
model

optimizer = optim.SGD(model.parameters(), lr=0.00001)

epochs = 20000

for epoch in range(epochs + 1):
  y_pred = model(X_train)
  loss = nn.MSELoss()(y_pred, y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  if epoch % 100 == 0:
      print(f'Epoch: {epoch}/{epochs} Loss: {loss:.6f}')

list(model.parameters()) # W: 0.6814, 0.8616, 1.3889 b: -0.2950

x_test = torch.FloatTensor([[100, 100, 100]])
y_pred = model(x_test)
y_pred

"""# **4. temps.csv 데이터에서 기온에 따른 지면 온도 예측하기**"""

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/컴퓨터비전_시즌2/4. 딥러닝 기초/data/temps.csv')
df

df.info()

df.isnull().mean()

df = df.dropna()

df.isnull().mean()

x_data = df[['기온(°C)']]
y_data = df[['지면온도(°C)']]

x_data = torch.FloatTensor(x_data.values)
y_data = torch.FloatTensor(y_data.values)
print(x_data.shape)
print(y_data.shape)

plt.figure(figsize=(8, 6))
plt.scatter(x_data, y_data)

model = nn.Linear(1, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
list(model.parameters())

epochs = 10000

for epoch in range(epochs + 1):
  y_pred = model(x_data)
  loss = nn.MSELoss()(y_pred, y_data)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  if epoch % 100 == 0:
    print(f'Epoch: {epoch}/{epochs} Loss: {loss:.6f}')

list(model.parameters()) # W:1.0854 b:0.8198

y_pred = model(x_data) # 따로 빼놓은 데이터가 없어서 일단 씀
y_pred

plt.figure(figsize=(8, 6))
plt.scatter(x_data, y_data)
plt.scatter(x_data, y_pred.detach().numpy())
# RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.

result = model(torch.FloatTensor([[40]]))
result

